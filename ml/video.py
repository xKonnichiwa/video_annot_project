import cv2  # Импортируем библиотеку OpenCV для обработки изображений и видео
import numpy as np  # Импортируем библиотеку NumPy для работы с массивами и числовыми данными
import tensorflow as tf  # Импортируем TensorFlow для работы с нейронными сетями и глубоким обучением
from tensorflow.keras.preprocessing import image  # Импортируем модуль для предварительной обработки изображений
from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions
# Импортируем функции для работы с предобученной моделью InceptionV3:
# - `preprocess_input` — для предварительной обработки изображения перед подачей в модель
# - `decode_predictions` — для декодирования результатов предсказания модели

from ultralytics import YOLO  # Импортируем пакет для использования предобученных моделей YOLO (например, YOLOv8) для детектирования объектов
import os  # Импортируем стандартный модуль os для работы с файловой системой
import json  # Импортируем модуль json для работы с JSON-файлами (чтение и запись)
import argparse  # Импортируем модуль argparse для обработки аргументов командной строки

from fer import FER  # Импортируем класс FER из библиотеки `fer` для распознавания эмоций на лицах


# Загрузка предобученной модели YOLOv8
yolo_model = YOLO('yolov8n.pt')

# Загрузка предобученной модели InceptionV3
inception_model = tf.keras.applications.InceptionV3(weights='imagenet')

def load_deeplab_model():
    """
    Загрузка предобученной модели сегментации.

    Описание:
    Эта функция загружает предобученную модель `DenseNet201` без верхних слоев (без полносвязного классификационного слоя).
    Вместо DeepLabv3+ используется модель `DenseNet201` из библиотеки Keras с параметром `include_top=False`.
    
    Аргументы:
    Функция не принимает аргументов.

    Возвращает:
    model — экземпляр предобученной модели `DenseNet201`, загруженной с весами `ImageNet`.
    Модель будет использоваться для задач классификации или сегментации.
    
    Примечание:
    Если требуется загрузить именно DeepLabv3+, модель должна быть импортирована из другой библиотеки, например, `tensorflow_models`.
    """
    
    # --- Загрузка модели DenseNet201 без верхних слоев ---
    
    model = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False)
    
    # `weights='imagenet'` — указываем, что загружаем модель с предобученными весами на базе данных ImageNet.
    # `include_top=False` — исключаем верхние слои модели, так как они предназначены для классификации.
    # Это позволяет использовать модель для других задач, таких как сегментация или извлечение признаков.

    return model  # Возвращаем загруженную модель

# Инициализация моделей POI
emotion_detector = FER(mtcnn=True)  # Используем MTCNN для детекции лиц

# Инициализация фонового субтрактор и салентного детектора
back_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)
saliency_detector = cv2.saliency.StaticSaliencySpectralResidual_create()

def detect_objects(frame):
    """
    Выполняет детектирование объектов на заданном кадре и возвращает аннотированное изображение
    и список детекций с информацией о каждом найденном объекте.

    Аргументы:
    frame — изображение в формате NumPy массива, на котором будет выполняться детектирование.

    Возвращает:
    - annotated_frame — изображение (кадр) с аннотациями (прямоугольниками и метками объектов).
    - detections — список детекций, где каждая детекция представлена как словарь с полями:
        - 'class': название класса объекта (например, 'person', 'car').
        - 'confidence': уверенность модели в данном предсказании (от 0 до 1).
        - 'bbox': координаты ограничивающего прямоугольника (xmin, ymin, xmax, ymax).
        - 'area': площадь ограничивающего прямоугольника (в пикселях).
    """

    # --- Выполнение детектирования объектов на кадре с помощью YOLO ---
    
    results = yolo_model(frame)  # Запускаем модель YOLO на входном кадре `frame`
    
    # `results` — это список результатов, и `results[0]` содержит данные для текущего кадра:
    # - `plot()` — функция для визуализации детекций на изображении (аннотирование кадра).
    # - `boxes.xyxy` — координаты ограничивающих прямоугольников (формат: [xmin, ymin, xmax, ymax]).
    # - `boxes.conf` — уверенности предсказания для каждого прямоугольника.
    # - `boxes.cls` — идентификаторы предсказанных классов объектов (например, 0 — 'person', 1 — 'car').

    annotated_frame = results[0].plot()  # Создаем аннотированное изображение с выделенными объектами

    # --- Извлечение данных детекций из результата модели ---
    
    boxes = results[0].boxes.xyxy.numpy()  # Координаты ограничивающих прямоугольников (в формате NumPy массива)
    confidences = results[0].boxes.conf.numpy()  # Уверенности для каждого объекта (NumPy массив)
    class_ids = results[0].boxes.cls.numpy()  # Идентификаторы классов объектов (NumPy массив)

    # --- Создание списка для хранения детекций ---
    
    detections = []  # Пустой список для хранения всех детекций в текущем кадре

    # --- Проход по каждой детекции и сбор информации ---
    
    for i, box in enumerate(boxes):
        # Распаковываем координаты ограничивающего прямоугольника (xmin, ymin, xmax, ymax)
        xmin, ymin, xmax, ymax = box

        # Извлекаем соответствующую уверенность и класс объекта
        confidence = confidences[i]
        cls = int(class_ids[i])  # Преобразуем идентификатор класса в целое число

        # Извлекаем метку класса из модели YOLO
        label = yolo_model.names[cls]  # Преобразуем идентификатор класса в название (например, 'person')

        # --- Рассчитываем дополнительные метрики для каждого объекта ---
        
        # Рассчитываем ширину и высоту ограничивающего прямоугольника
        width = xmax - xmin
        height = ymax - ymin
        
        # Рассчитываем площадь ограничивающего прямоугольника
        area = width * height  # Площадь в пикселях

        # --- Добавляем информацию о текущем объекте в список детекций ---
        
        detections.append({
            'class': label,  # Название класса объекта (например, 'car')
            'confidence': float(confidence),  # Уверенность в предсказании (например, 0.85)
            'bbox': [float(xmin), float(ymin), float(xmax), float(ymax)],  # Координаты прямоугольника
            'area': float(area)  # Площадь ограничивающего прямоугольника
        })

    # --- Возвращение аннотированного кадра и списка детекций ---
    
    return annotated_frame, detections

def analyze_events(frame):
    """
    Выполняет анализ событий на кадре (изображении) с использованием предобученной модели InceptionV3.
    Используется для классификации изображений, возвращая наиболее вероятные метки и вероятности.

    Аргументы:
    frame — изображение в формате NumPy массива, которое будет подано на вход модели для анализа.

    Возвращает:
    events — список словарей, содержащих предсказания модели:
        - 'class_id': идентификатор класса (например, 'n02834778' — идентификатор класса по WordNet).
        - 'name': название предсказанного класса (например, 'bicycle').
        - 'probability': вероятность, с которой изображение принадлежит данному классу.
    
    Пример:
    Если входное изображение содержит велосипед, модель может вернуть:
    [{'class_id': 'n02834778', 'name': 'bicycle', 'probability': 0.85}, ...]
    """

    # --- Предобработка изображения перед передачей в модель InceptionV3 ---

    # Изменяем размер изображения до (299, 299), так как InceptionV3 ожидает этот размер на входе
    img = cv2.resize(frame, (299, 299))

    # Преобразуем изображение в формат массива, который ожидает Keras
    img_array = image.img_to_array(img)  # Преобразуем изображение в массив (299, 299, 3)
    
    # Добавляем дополнительное измерение, чтобы форма массива соответствовала входу модели
    img_array = np.expand_dims(img_array, axis=0)  # Новая форма: (1, 299, 299, 3)

    # Применяем предобработку, специфичную для модели InceptionV3 (например, нормализация)
    img_array = preprocess_input(img_array)  # Нормализация данных (используется метод из Keras)

    # --- Выполнение предсказания с использованием модели InceptionV3 ---

    predictions = inception_model.predict(img_array)  # Получаем предсказания от модели InceptionV3
    # `predictions` — массив вероятностей для всех классов ImageNet (например, [0.001, 0.005, ...])

    # Декодируем предсказания в понятные метки классов (например, ['bicycle', 'car'])
    decoded_predictions = decode_predictions(predictions, top=3)[0]
    # `decode_predictions` преобразует массив вероятностей в список кортежей (класс, метка, вероятность)
    # `top=3` — выбираем три наиболее вероятных предсказания

    # --- Формирование списка предсказанных событий ---

    # Создаем список словарей, где каждый словарь содержит информацию о предсказанном классе:
    # - 'class_id': идентификатор класса по базе данных WordNet (например, 'n02834778')
    # - 'name': название предсказанного класса (например, 'bicycle')
    # - 'probability': вероятность, с которой модель считает, что изображение принадлежит данному классу
    events = [
        {'class_id': label, 'name': name, 'probability': float(prob)}
        for label, name, prob in decoded_predictions
    ]

    # --- Возвращение результатов анализа ---
    
    return events  # Возвращаем список событий с вероятностями

def segment_scenes(frame, model):
    """
    Выполняет сегментацию сцены на изображении с использованием заданной модели сегментации.
    
    Аргументы:
    frame — входное изображение в формате NumPy массива (например, кадр видео или изображение).
    model — модель сегментации, обученная для разделения изображений на классы (например, DeepLabv3+).

    Возвращает:
    colored_segmentation — изображение, где каждому сегменту присвоен уникальный цвет (RGB).
    
    Описание:
    - Функция изменяет размер входного изображения, чтобы соответствовать ожиданиям модели.
    - Выполняется предсказание сегментации и последующая обработка, чтобы преобразовать выход модели в цветное изображение.
    - Цвет каждого сегмента выбирается случайным образом из цветовой карты (RGB).

    Примечание:
    Функция полезна для визуализации результатов сегментации, так как каждому классу на изображении присваивается уникальный цвет.
    """
    
    # --- Изменение размера изображения для соответствия модели ---
    
    # Изменяем размер входного изображения `frame` до (512, 512), чтобы соответствовать ожидаемым входным данным модели
    img = cv2.resize(frame, (512, 512))  # Изменение размера изображения до (512, 512)
    
    # --- Предобработка изображения для подачи в модель ---
    
    # Добавляем дополнительное измерение, чтобы указать модель, что это единичный пример (1, 512, 512, 3)
    img_array = np.expand_dims(img, axis=0)
    
    # Преобразуем изображение в тип данных `tf.uint8` (целочисленные значения от 0 до 255)
    img_array = tf.image.convert_image_dtype(img_array, dtype=tf.uint8)

    # --- Выполнение предсказания с использованием модели сегментации ---
    
    # Запускаем предсказание на модели, результат — массив предсказанных классов для каждого пикселя
    predictions = model.predict(img_array)  # Модель возвращает предсказания для каждого пикселя в формате (1, 512, 512, num_classes)

    # Находим индекс класса с максимальной вероятностью для каждого пикселя (например, [0, 1, 0, 2, ...])
    segmented_image = tf.argmax(predictions[0], axis=-1)  # Получаем метки классов по оси последнего измерения (-1)
    
    # Преобразуем `segmented_image` в формат (512, 512, 1), чтобы сохранить структуру изображения
    segmented_image = np.expand_dims(segmented_image, axis=-1)  # Добавляем измерение для согласованности формы массива

    # --- Изменение размера сегментированного изображения до исходного размера ---
    
    # Преобразуем предсказанные метки классов обратно в исходный размер изображения `frame`
    segmented_image = tf.image.resize(segmented_image, (frame.shape[0], frame.shape[1]), method='nearest')
    
    # Преобразуем результат в формат NumPy массива типа uint8 для дальнейшей обработки
    segmented_image = np.array(segmented_image, dtype=np.uint8)

    # --- Создание цветовой карты для визуализации сегментации ---

    # Определяем количество уникальных классов в сегментированном изображении
    num_classes = np.max(segmented_image) + 1  # Добавляем 1, чтобы учесть все классы (например, если max = 20, то num_classes = 21)

    # Генерируем случайную цветовую карту (RGB) для каждого класса
    # `size=(num_classes, 3)` указывает на создание массива формы (num_classes, 3) — по 3 значения RGB на каждый класс
    color_map = np.random.randint(0, 255, size=(num_classes, 3), dtype=np.uint8)

    # --- Проверка на выход класса за пределы ожидаемого диапазона ---
    
    if np.max(segmented_image) >= num_classes:
        # Если метка класса превышает допустимое количество классов, выбрасываем исключение
        raise ValueError(f"Detected class index out of bounds. Expected less than {num_classes}, but found {np.max(segmented_image)}.")

    # --- Применение цветовой карты к сегментированному изображению ---
    
    # Применяем цветовую карту, чтобы каждому пикселю присвоить уникальный цвет на основе его метки класса
    # `segmented_image.squeeze()` убирает лишнее измерение, чтобы получить массив формы (height, width)
    colored_segmentation = color_map[segmented_image.squeeze()]  # Преобразование меток в цвета на основе color_map

    # --- Возвращение результата ---
    
    return colored_segmentation  # Возвращаем цветное сегментированное изображение

def load_json_data(file_path):
    """
    Загружает данные из JSON файла, если файл существует.

    Аргументы:
    file_path — строка, представляющая путь к JSON файлу, который нужно загрузить.

    Возвращает:
    - Если файл существует и успешно прочитан, возвращает данные, загруженные из JSON.
    - Если файл не существует или чтение не удалось, возвращает пустой словарь `{}`.
    
    Пример:
    - Для файла "data.json" с содержимым {"key": "value"} функция вернет: {'key': 'value'}.
    - Если файл не существует, функция вернет пустой словарь: {}.
    """
    
    # --- Проверка существования файла по указанному пути ---
    
    if os.path.exists(file_path):  # Проверяем, существует ли файл по указанному пути
        # Если файл существует, открываем его для чтения
        with open(file_path, 'r', encoding='utf-8') as f:
            # --- Чтение и загрузка данных из JSON ---
            
            return json.load(f)  # Загружаем содержимое файла как Python объект (например, словарь или список)

    # --- Если файл не существует, возвращаем пустой словарь ---
    
    return {}  # Если файл отсутствует, возвращаем пустой словарь

def save_results_to_json(video_name, scene_data, json_output_file):
    """
    Сохраняет результаты анализа видео в указанный JSON файл.
    
    Если файл уже существует, данные по текущему видео будут обновлены или добавлены.
    
    Аргументы:
    video_name — строка, представляющая название видео, используемое в качестве ключа в JSON.
                 Пример: "example_video.mp4".
    scene_data — словарь с результатами анализа видео (например, информация о сценах или детекциях).
                 Пример: {'scene_1': {'objects': ['person', 'car'], 'events': ['walking']}, ...}.
    json_output_file — строка, представляющая путь к выходному JSON файлу.
                       Пример: "output_results.json".
    
    Возвращает:
    Ничего не возвращает, но сохраняет или обновляет данные в указанном JSON файле.
    
    Пример:
    Если файл "output_results.json" уже содержит данные для других видео, функция добавит или обновит данные по текущему видео.
    """

    # --- Шаг 1: Загрузка существующих данных из JSON файла ---
    
    data = load_json_data(json_output_file)  # Загружаем существующие данные из указанного файла JSON
    
    # --- Шаг 2: Обновление или добавление новых данных для текущего видео ---
    
    # Обновляем или добавляем результаты анализа для текущего видео
    # Например, если `video_name = "example_video.mp4"`, а `scene_data = {'scene_1': ...}`, то:
    # data = {'example_video.mp4': {'scene_1': ...}, ...}
    data[video_name] = scene_data

    # --- Шаг 3: Сохранение обновленных данных обратно в JSON файл ---
    
    # Открываем выходной файл в режиме записи ('w') и сохраняем данные в формате JSON
    with open(json_output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        # Параметры:
        # - ensure_ascii=False: сохраняет символы юникода в их естественном представлении (например, кириллицу)
        # - indent=4: форматирует JSON с отступом в 4 пробела, делая его читаемым

    # --- Шаг 4: Вывод сообщения об успешном сохранении ---
    
    print(f"Результаты для {video_name} успешно сохранены в {json_output_file}")
    # Выводим сообщение, подтверждающее успешное сохранение результатов в файл

def detect_faces_and_emotions(frame):
    """
    Выполняет детекцию лиц на изображении и распознавание эмоций с использованием библиотеки FER.

    Аргументы:
    frame — изображение в формате NumPy массива, на котором нужно обнаружить лица и определить эмоции.

    Возвращает:
    - face_data — список словарей, содержащих информацию о каждом обнаруженном лице:
        - 'bbox': координаты ограничивающего прямоугольника для лица в формате [x, y, w, h].
        - 'area': площадь ограничивающего прямоугольника лица (в пикселях).
        - 'emotion': основной тип эмоции с вероятностью (например, {'label': 'happy', 'probability': 0.85}).
        
    - bounding_boxes — список координат для каждого лица в формате [x, y, w, h].
    
    Пример:
    Для входного кадра с двумя лицами и соответствующими эмоциями функция может вернуть:
    face_data = [
        {'bbox': [100, 50, 80, 80], 'area': 6400, 'emotion': {'label': 'happy', 'probability': 0.85}},
        {'bbox': [200, 100, 90, 90], 'area': 8100, 'emotion': {'label': 'neutral', 'probability': 0.65}}
    ]
    bounding_boxes = [[100, 50, 80, 80], [200, 100, 90, 90]]
    """

    # --- Детекция лиц и эмоций с использованием модели FER ---
    
    emotions = emotion_detector.detect_emotions(frame)  # Запускаем детекцию лиц и распознавание эмоций на кадре `frame`
    # `emotions` — список, содержащий информацию обо всех обнаруженных лицах и их эмоциях
    
    face_data = []  # Пустой список для хранения информации обо всех обнаруженных лицах

    # --- Проход по каждому обнаруженному лицу и сбор данных ---
    
    for face in emotions:
        # Получаем координаты ограничивающего прямоугольника для лица
        # face["box"] возвращает кортеж (x, y, w, h)
        (x, y, w, h) = face["box"]

        # --- Рассчитываем площадь ограничивающего прямоугольника ---
        
        area = w * h  # Площадь ограничивающего прямоугольника (ширина * высота)

        # --- Определение основной эмоции и ее вероятности ---
        
        if face["emotions"]:
            # `face["emotions"]` — это словарь эмоций и их вероятностей (например, {'happy': 0.75, 'sad': 0.15, ...})
            
            # Определяем эмоцию с максимальной вероятностью
            top_emotion = max(face["emotions"], key=face["emotions"].get)  # Находим ключ (эмоцию) с максимальным значением (вероятностью)
            
            # Извлекаем вероятность для этой эмоции
            emotion_probability = face["emotions"][top_emotion]  # Вероятность для основной эмоции (например, 0.85)
            
            # Создаем словарь с меткой и вероятностью основной эмоции
            emotion = {
                'label': top_emotion,  # Метка эмоции (например, 'happy')
                'probability': float(emotion_probability)  # Вероятность (например, 0.85)
            }
        else:
            # Если эмоции не определены, устанавливаем значение None
            emotion = None

        # --- Сохраняем информацию о текущем лице в виде словаря ---
        
        face_info = {
            'bbox': [int(x), int(y), int(w), int(h)],  # Координаты ограничивающего прямоугольника лица (x, y, w, h)
            'area': float(area),  # Площадь ограничивающего прямоугольника (в пикселях)
            'emotion': emotion  # Основная эмоция и вероятность
        }

        # Добавляем информацию о лице в общий список
        face_data.append(face_info)

    # --- Возвращение результатов ---
    
    # Первый возвращаемый список — `face_data` со всей информацией о лицах и эмоциях.
    # Второй возвращаемый список — `bounding_boxes`, содержащий только координаты лиц.
    return face_data, [f["box"] for f in emotions]

def detect_moving_objects(frame, back_subtractor):
    """
    Детектирует движущиеся объекты на заданном кадре, используя метод вычитания фона.

    Аргументы:
    frame — изображение в формате NumPy массива (например, текущий кадр из видеопотока).
    back_subtractor — объект фонового субтрактора (например, cv2.createBackgroundSubtractorMOG2()).
                      Используется для выделения движущихся объектов путем вычитания текущего фона.

    Возвращает:
    - moving_objects — список словарей, содержащих информацию о каждом движущемся объекте:
        - 'bbox': координаты ограничивающего прямоугольника для объекта в формате [x, y, w, h].
        - 'area': площадь ограничивающего прямоугольника объекта (в пикселях).
    
    - fg_mask — маска переднего плана (движущихся объектов) в формате NumPy массива, где:
        - 0 — фон
        - 255 — движущийся объект

    Пример:
    Если в кадре обнаружены движущиеся объекты, функция может вернуть:
    moving_objects = [
        {'bbox': [50, 100, 80, 60], 'area': 4800},
        {'bbox': [150, 200, 100, 80], 'area': 8000}
    ]
    fg_mask — бинарная маска того же размера, что и `frame`, где 255 обозначает движущиеся объекты.
    """

    # --- Шаг 1: Применение вычитания фона для выделения движущихся объектов ---

    # `back_subtractor.apply(frame)` применяет алгоритм вычитания фона к текущему кадру `frame`
    # `fg_mask` — это бинарная маска переднего плана, где 0 обозначает фон, а 255 — движущийся объект
    fg_mask = back_subtractor.apply(frame)

    # --- Шаг 2: Применение морфологических операций для улучшения качества маски ---
    
    # Создаем структурирующий элемент в виде эллипса (размер 3x3) для морфологических операций
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    
    # Применяем операцию морфологического открытия (удаление мелких шумов и артефактов)
    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel, iterations=2)

    # Применяем расширение (dilate), чтобы заполнить внутренние разрывы в движущихся объектах
    fg_mask = cv2.dilate(fg_mask, kernel, iterations=2)

    # --- Шаг 3: Поиск контуров в бинарной маске переднего плана ---

    # `cv2.findContours` ищет контуры объектов на маске `fg_mask`
    # `cv2.RETR_EXTERNAL` — извлекает только внешние контуры
    # `cv2.CHAIN_APPROX_SIMPLE` — упрощает контуры, сохраняя только ключевые точки
    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # --- Шаг 4: Фильтрация контуров и сохранение информации о движущихся объектах ---
    
    moving_objects = []  # Пустой список для хранения информации обо всех обнаруженных движущихся объектах

    for contour in contours:
        # Фильтрация мелких объектов: пропускаем контуры, если площадь меньше 500 пикселей
        if cv2.contourArea(contour) < 500:
            continue

        # --- Определение ограничивающего прямоугольника для текущего контура ---
        
        # `cv2.boundingRect(contour)` вычисляет минимальный ограничивающий прямоугольник, охватывающий контур
        x, y, w, h = cv2.boundingRect(contour)

        # Рассчитываем площадь ограничивающего прямоугольника
        area = w * h

        # --- Сохранение информации о движущемся объекте ---
        
        moving_objects.append({
            'bbox': [int(x), int(y), int(w), int(h)],  # Координаты и размер прямоугольника: [x, y, ширина, высота]
            'area': float(area)  # Площадь ограничивающего прямоугольника в пикселях
        })

    # --- Возвращение списка движущихся объектов и маски переднего плана ---
    
    return moving_objects, fg_mask  # Возвращаем список объектов и маску переднего плана

def detect_salient_regions(frame):
    """
    Выявляет салентные зоны в кадре с использованием детектора салентности.
    
    Аргументы:
    frame — изображение в формате NumPy массива, на котором необходимо выделить салентные зоны.

    Возвращает:
    - salient_regions — список словарей, каждый из которых содержит информацию о салентных зонах:
        - 'bbox': координаты ограничивающего прямоугольника в формате [x, y, w, h].
        - 'area': площадь ограничивающего прямоугольника в пикселях.

    - saliency_map — бинарная карта салентности, где 255 — салентные области, а 0 — остальные области изображения.

    Пример:
    Если на кадре обнаружены несколько салентных зон, функция может вернуть:
    salient_regions = [
        {'bbox': [50, 100, 80, 60], 'area': 4800},
        {'bbox': [150, 200, 100, 80], 'area': 8000}
    ]
    saliency_map — бинарная карта того же размера, что и исходный кадр.
    """
    
    # --- Шаг 1: Вычисление карты салентности ---
    
    # Используем объект `saliency_detector` для вычисления карты салентности кадра `frame`
    # `saliency_map` — карта салентности, значения которой варьируются от 0 до 1.
    success, saliency_map = saliency_detector.computeSaliency(frame)

    # Проверка, удалось ли вычислить карту салентности
    if not success:
        return []  # Возвращаем пустой список, если не удалось выполнить вычисление

    # --- Шаг 2: Преобразование карты салентности в бинарное изображение ---
    
    # Преобразуем карту салентности из диапазона [0, 1] в диапазон [0, 255], тип данных — uint8
    saliency_map = (saliency_map * 255).astype("uint8")

    # Применяем бинаризацию с использованием порога, который автоматически определяется методом Otsu
    _, saliency_map = cv2.threshold(saliency_map, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

    # --- Шаг 3: Поиск контуров на бинарной карте салентности ---
    
    # Ищем контуры в бинарной карте салентности, используя только внешние контуры
    contours, _ = cv2.findContours(saliency_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # --- Шаг 4: Фильтрация контуров и извлечение информации о салентных регионах ---
    
    salient_regions = []  # Пустой список для хранения информации о всех салентных регионах

    for contour in contours:
        # Фильтрация мелких областей: пропускаем области с площадью меньше 1000 пикселей
        if cv2.contourArea(contour) < 1000:
            continue

        # Определение ограничивающего прямоугольника для текущего контура
        x, y, w, h = cv2.boundingRect(contour)

        # Рассчет площади ограничивающего прямоугольника
        area = w * h

        # Сохранение информации о салентной зоне в виде словаря
        salient_regions.append({
            'bbox': [int(x), int(y), int(w), int(h)],  # Координаты прямоугольника: x, y, ширина, высота
            'area': float(area)  # Площадь области в пикселях
        })

    # --- Возвращение списка салентных зон и карты салентности ---
    
    return salient_regions, saliency_map  # Возвращаем список регионов и карту салентности

def visualize_heatmap_zones(image, detections, faces, moving_objects, salient_regions):
    """
    Визуализирует различные объекты, лица, движущиеся объекты и салентные зоны на входном изображении.

    Аргументы:
    image — входное изображение в формате NumPy массива, на котором будут визуализироваться зоны.
    detections — список объектов, обнаруженных на изображении (например, с использованием YOLO), содержащих поля:
        - 'bbox': координаты ограничивающего прямоугольника объекта в формате [xmin, ymin, xmax, ymax].
        - 'area': площадь объекта в пикселях.
        - 'class': название класса объекта (например, 'person', 'car').
        - 'confidence': уверенность модели в предсказании (от 0 до 1).
    faces — список лиц, обнаруженных на изображении, содержащих поля:
        - 'bbox': координаты ограничивающего прямоугольника лица в формате [x, y, w, h].
        - 'area': площадь зоны лица.
        - 'emotion': основная эмоция с вероятностью, например, {'label': 'happy', 'probability': 0.85}, или None.
    moving_objects — список движущихся объектов, содержащих поля:
        - 'bbox': координаты ограничивающего прямоугольника движущегося объекта.
        - 'area': площадь объекта.
    salient_regions — список салентных зон, содержащих поля:
        - 'bbox': координаты ограничивающего прямоугольника салентной зоны.
        - 'area': площадь зоны.

    Возвращает:
    image — изображение с аннотациями всех переданных зон.
    
    Описание:
    - Прямоугольники рисуются для всех обнаруженных объектов, лиц, движущихся объектов и салентных зон.
    - Над каждым прямоугольником отображается текстовая информация (класс объекта, эмоция, площадь).
    """

    # --- Визуализация обнаруженных объектов (обычно с помощью детектора, такого как YOLO) ---
    
    for detection in detections:
        xmin, ymin, xmax, ymax = map(int, detection['bbox'])  # Координаты ограничивающего прямоугольника объекта
        area = detection['area']  # Площадь объекта
        # Формируем текстовую метку с названием класса, уверенностью и площадью объекта
        label = f"{detection['class']} ({detection['confidence']*100:.1f}%) Area: {int(area)}"
        # Рисуем прямоугольник вокруг объекта зеленого цвета
        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
        # Отображаем текстовую метку над объектом
        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # --- Визуализация лиц, включая эмоции, если они распознаны ---
    
    for face in faces:
        x, y, w, h = face['bbox']  # Координаты лица
        area = face['area']  # Площадь лица
        emotion = face['emotion']  # Основная эмоция (если доступна)
        
        # Если эмоция распознана, отображаем метку эмоции, вероятность и площадь лица
        if emotion:
            label = f"{emotion['label']} ({emotion['probability']*100:.1f}%) Area: {int(area)}"
            color = (255, 0, 0)  # Синий цвет для лиц с эмоциями
        else:
            label = f"Area: {int(area)}"  # Если эмоция не распознана, отображаем только площадь
            color = (0, 0, 255)  # Красный цвет для лиц без эмоций
        
        # Рисуем прямоугольник вокруг лица
        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
        # Отображаем текстовую метку над лицом
        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # --- Визуализация движущихся объектов ---
    
    for mo in moving_objects:
        x, y, w, h = mo['bbox']  # Координаты движущегося объекта
        area = mo['area']  # Площадь движущегося объекта
        # Формируем метку с указанием, что это движущийся объект, и его площади
        label = f"Moving Object Area: {int(area)}"
        # Рисуем прямоугольник вокруг движущегося объекта желтого цвета
        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 255), 2)
        # Отображаем текстовую метку над объектом
        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)

    # --- Визуализация салентных зон ---
    
    for sr in salient_regions:
        x, y, w, h = sr['bbox']  # Координаты салентной зоны
        area = sr['area']  # Площадь салентной зоны
        # Формируем метку с указанием площади салентной зоны
        label = f"Salient Area: {int(area)}"
        # Рисуем прямоугольник вокруг салентной зоны желто-голубого цвета
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 0), 2)
        # Отображаем текстовую метку над зоной
        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)

    # --- Возвращение изображения с аннотациями ---
    
    return image  # Возвращаем изображение с визуализированными зонами


def process_video(video_path, json_output_path, scene_change_threshold=0.5, process_every_100_frames=False):
    """
    Выполняет обработку видео для выявления сцен, объектов, лиц, движущихся объектов и салентных зон.
    Результаты сохраняются в JSON файл, а сегментированные сцены сохраняются в виде отдельных видеофайлов.

    Аргументы:
    video_path — путь к входному видеофайлу.
    json_output_path — путь к выходному JSON файлу, в который сохраняются результаты анализа.
    scene_change_threshold — порог для детекции смены сцены, основанный на разнице гистограмм (по умолчанию 0.5).
    process_every_100_frames — флаг, указывающий, обрабатывать ли только каждый 100-й кадр (по умолчанию False).
    
    Описание:
    - Видеопоток анализируется на наличие смен сцен на основе сравнения гистограмм кадров.
    - Обнаруживаются объекты, лица, движущиеся объекты и салентные зоны.
    - Визуализированные результаты и сегментированные сцены сохраняются в выходные файлы.
    """

    # --- Шаг 1: Инициализация видео и моделей ---
    
    cap = cv2.VideoCapture(video_path)  # Открываем видеопоток
    deeplab_model = load_deeplab_model()  # Загружаем модель DeepLab для сегментации

    scene_index = 0  # Индекс текущей сцены
    scene_output_path = f'scenes/scene_{scene_index}.mp4'  # Путь для сохранения текущей сцены

    # Инициализация объекта VideoWriter для записи видео (кодек mp4v)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(scene_output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    video_name = os.path.splitext(os.path.basename(video_path))[0]  # Имя видео для использования в выходных данных
    scene_data = []  # Список для хранения данных анализа по каждой сцене

    frame_counter = 0  # Счетчик кадров
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Общее количество кадров в видео

    # --- Шаг 2: Основной цикл обработки видео ---
    
    while cap.isOpened():
        ret, frame = cap.read()  # Чтение текущего кадра
        if not ret:
            break  # Если кадр не прочитан, выходим из цикла

        frame_counter += 1

        # --- Шаг 3: Пропуск кадров, если включен режим обработки только 100-х кадров ---
        
        if process_every_100_frames:
            if frame_counter % 100 != 0:
                continue  # Пропускаем кадры, если это не каждый 100-й кадр
        else:
            # Если режим обработки каждого 100-го кадра не включен, обрабатываем первый, последний и средний кадры
            if frame_counter != 1 and frame_counter != total_frames and frame_counter != total_frames // 2:
                continue
        
        print(f"Processing key frame: {frame_counter}")

        # --- Шаг 5: Детектирование объектов, событий, лиц и прочего ---
        
        object_detected_frame, detections = detect_objects(frame)  # Детектирование объектов
        event_predictions = analyze_events(frame)  # Анализ событий на изображении
        segmented_frame = segment_scenes(frame, deeplab_model)  # Сегментация изображения с помощью модели DeepLab
        faces, face_boxes = detect_faces_and_emotions(frame)  # Детектирование лиц и эмоций
        moving_objects, fg_mask = detect_moving_objects(frame, back_subtractor)  # Обнаружение движущихся объектов
        salient_regions, saliency_map = detect_salient_regions(frame)  # Выявление салентных зон

        # --- Шаг 6: Визуализация результатов ---
        
        annotated_frame = visualize_heatmap_zones(
            object_detected_frame.copy(),
            detections,
            faces,
            moving_objects,
            salient_regions
        )  # Визуализируем объекты, лица, движущиеся объекты и салентные зоны

        # Записываем аннотированный кадр и сегментированный кадр в текущий файл сцены
        out.write(cv2.hconcat([annotated_frame, segmented_frame]))

        # --- Шаг 7: Сохранение данных по кадрам ---
        
        scene_data.append({
            'scene': scene_index,
            'frame': int(cap.get(cv2.CAP_PROP_POS_FRAMES)),  # Номер текущего кадра
            'detections': detections,  # Обнаруженные объекты
            'events': event_predictions,  # Прогнозируемые события
            'poi': {  # Points of Interest (ключевые объекты)
                'faces': faces,
                'moving_objects': moving_objects,
                'salient_regions': salient_regions
            }
        })

        # --- Шаг 8: Отображение результатов ---
        
        combined_display = cv2.hconcat([annotated_frame, segmented_frame])  # Комбинируем кадры для отображения
        cv2.imshow('Object Detection, Segmentation and POI', combined_display)

        if cv2.waitKey(1) & 0xFF == ord('q'):  # Нажмите 'q', чтобы выйти
            break

    # --- Шаг 9: Завершение процесса ---
    
    cap.release()  # Закрываем видеопоток
    out.release()  # Закрываем последний видеофайл
    cv2.destroyAllWindows()  # Закрываем все окна OpenCV

    # Сохраняем все данные анализа в JSON файл
    save_results_to_json(video_name, scene_data, json_output_path)


if __name__ == "__main__":
    """
    Основной блок для запуска обработки видео с использованием аргументов командной строки.
    """
    
    # --- Шаг 1: Настройка парсинга аргументов командной строки ---
    
    parser = argparse.ArgumentParser(description='Video Processing Module with POI Detection')
    # Аргумент 'video_path' — строка, представляющая путь к входному видеофайлу
    parser.add_argument('video_path', type=str, help='Path to the input video file')
    # Аргумент 'json_output_path' — строка, представляющая путь к выходному JSON файлу
    parser.add_argument('json_output_path', type=str, help='Path to the output JSON file')
    
    # --- Шаг 2: Получение аргументов ---
    
    args = parser.parse_args()  # Получаем аргументы командной строки
    
    # --- Шаг 3: Проверка и создание папки для сохранения сцен ---
    
    # Проверяем, существует ли папка 'scenes', где будут сохранены видео сцен
    if not os.path.exists('scenes'):
        os.makedirs('scenes')  # Создаем папку 'scenes', если она не существует

    # --- Шаг 4: Запуск обработки видео ---
    
    # Вызываем функцию `process_video`, передавая путь к видео и путь для сохранения JSON файла
    process_video(args.video_path, args.json_output_path)
