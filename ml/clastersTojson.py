import json  # Импорт модуля для работы с JSON-форматом (чтение и запись данных)
import numpy as np  # Импорт библиотеки для работы с многомерными массивами и математическими операциями
from sklearn.feature_extraction.text import TfidfVectorizer  # Импорт класса для создания TF-IDF векторизации текста
from sklearn.cluster import  AgglomerativeClustering  # Импорт классов для кластеризации данных
import matplotlib.pyplot as plt  # Импорт модуля для построения графиков и визуализации данных
from sklearn.metrics import silhouette_score  # Импорт метрики для оценки качества кластеризации (коэффициент силуэта)
from sklearn.decomposition import PCA  # Импорт класса для выполнения PCA (снижение размерности данных)
from sklearn.metrics import davies_bouldin_score  # Импорт метрики для оценки кластеризации (индекс Дэвиса-Болдена)



# Преобразуем все значения типа int32 в обычные int
def convert_to_serializable(obj):
    """
    Рекурсивная функция для преобразования объектов в сериализуемые типы данных.
    Используется для приведения типов данных, не поддерживаемых JSON, к стандартным типам.

    Аргументы:
    obj — объект любого типа (например, int, list, dict), который нужно сделать сериализуемым.

    Возвращает:
    Преобразованный объект, который может быть сериализован с помощью JSON.
    """
    
    # Если объект является типом numpy int32, то преобразуем его в стандартный int
    if isinstance(obj, np.int32):
        return int(obj)
    
    # Если объект — это словарь, то рекурсивно применяем функцию ко всем значениям словаря
    elif isinstance(obj, dict):
        return {k: convert_to_serializable(v) for k, v in obj.items()}  # Применяем функцию к каждому элементу словаря
    
    # Если объект — это список, то рекурсивно применяем функцию ко всем элементам списка
    elif isinstance(obj, list):
        return [convert_to_serializable(i) for i in obj]  # Применяем функцию к каждому элементу списка
    
    # Если объект не требует преобразования, возвращаем его без изменений
    return obj


# Функция для вычисления средних значений детекций объектов и событий по кадрам видео для данного шота
def get_average_video_data(video_shot_frames):
    """
    Вычисляет средние значения детекций объектов и вероятности событий по всем кадрам в шоте видео.

    Аргументы:
    video_shot_frames — список кадров (frames), в которых содержатся детекции объектов и события.
    
    Каждый кадр представлен как словарь со следующими ключами:
        - 'detections': список детекций объектов в кадре, где каждая детекция — это словарь вида {'class': <имя объекта>, 'confidence': <уверенность>}.
        - 'events': список событий в кадре, где каждое событие — это словарь вида {'name': <название события>, 'probability': <вероятность события>}.

    Возвращает:
    Два словаря:
    1. avg_detections — средняя уверенность (confidence) для каждого класса объектов в шоте, например, {'person': 0.95, 'car': 0.8}.
    2. avg_events — средняя вероятность (probability) для каждого типа события в шоте, например, {'running': 0.7, 'jumping': 0.6}.
    """
    
    # Инициализация пустых словарей для хранения детекций объектов и событий
    avg_detections = {}  # Для хранения средних значений уверенности по каждому классу объектов
    avg_events = {}  # Для хранения средних значений вероятностей по каждому типу событий
    
    # Проход по каждому кадру в шоте
    for frame in video_shot_frames:
        
        # Обработка детекций объектов в текущем кадре
        for detection in frame['detections']:
            obj_class = detection['class']  # Получаем класс объекта (например, 'person', 'car')
            
            # Если объект еще не добавлен в словарь, инициализируем пустым списком
            if obj_class not in avg_detections:
                avg_detections[obj_class] = []
                
            # Добавляем уверенность (confidence) текущей детекции в список
            avg_detections[obj_class].append(detection['confidence'])
        
        # Обработка событий в текущем кадре
        for event in frame['events']:
            event_name = event['name']  # Получаем название события (например, 'running', 'jumping')
            
            # Если событие еще не добавлено в словарь, инициализируем пустым списком
            if event_name not in avg_events:
                avg_events[event_name] = []
                
            # Добавляем вероятность (probability) текущего события в список
            avg_events[event_name].append(event['probability'])
    
    # Рассчитываем среднее значение уверенности для каждого класса объектов
    avg_detections = {obj: np.mean(confidences) for obj, confidences in avg_detections.items()}
    
    # Рассчитываем среднюю вероятность для каждого типа события
    avg_events = {event: np.mean(probabilities) for event, probabilities in avg_events.items()}
    
    # Возвращаем два словаря: средняя уверенность объектов и средняя вероятность событий
    return avg_detections, avg_events

# Функция для объединения данных аудио и видео по каждому шоту
def merge_shot_data(audio_shot, video_shot_frames):
    """
    Объединяет данные аудио и видео для конкретного шота и формирует его описание.

    Аргументы:
    audio_shot — словарь с данными анализа аудио для текущего шота.
    video_shot_frames — список кадров видео, содержащий детекции объектов и события.

    Возвращает:
    description — словарь, содержащий объединенное описание шота с характеристиками аудио и видео.

    Аудиоданные включают:
        - transcription — Текстовая транскрипция аудио.
        - sentiment — Оценка тональности (позитивная, нейтральная или негативная).
        - rms — Корневая среднеквадратическая амплитуда аудиосигнала.
        - spectral_centroid — Центр тяжести спектра аудиосигнала.
        - spectral_bandwidth — Ширина спектра аудиосигнала.
        - clap_analysis — Метки, описывающие события, классифицированные моделью CLAP.
        - labeled_transcriptions — Распознанные метки в транскрипциях.

    Видеоданные включают:
        - avg_video_objects — Средняя уверенность по детекциям объектов для каждого класса.
        - avg_events — Средняя вероятность событий для каждого типа.
        - moving_objects — Перечень движущихся объектов в формате: "Object at bbox <координаты> with area <площадь>".
    """

    # --- Извлечение ключевых характеристик из аудиоанализа ---
    
    # Извлекаем первую текстовую транскрипцию, если она существует, иначе ставим значение 'N/A'
    transcription = audio_shot['transcriptions'][0].get('text', 'N/A')
    
    # Извлекаем оценку тональности, если она доступна, иначе указываем 'NEUTRAL'
    sentiment = audio_shot['sentiment_analysis'][0].get('sentiment', 'NEUTRAL')
    
    # Извлекаем характеристики аудиосигнала: RMS, спектральный центр и ширину спектра
    rms = audio_shot['soundscape_analysis'].get('rms', 0)
    spectral_centroid = audio_shot['soundscape_analysis'].get('spectral_centroid', 0)
    spectral_bandwidth = audio_shot['soundscape_analysis'].get('spectral_bandwidth', 0)
    
    # Объединяем анализ CLAP (если метки событий есть) в одну строку
    clap_analysis = ', '.join(audio_shot.get('clap_analysis', []))
    
    # Объединяем все распознанные метки в транскрипциях
    labeled_transcriptions = ', '.join(audio_shot.get('labeled_transcriptions', []))
    
    # --- Извлечение средних характеристик из видеодетекций и событий ---
    
    # Вызываем функцию для расчета средних значений детекций объектов и вероятностей событий по кадрам
    avg_detections, avg_events = get_average_video_data(video_shot_frames)

    # --- Анализ движущихся объектов в каждом кадре ---
    
    # Инициализация списка для хранения описаний движущихся объектов
    moving_objects_list = []
    
    # Проход по всем кадрам в текущем шоте
    for frame in video_shot_frames:
        # Получаем список движущихся объектов из каждого кадра (ключ 'moving_objects')
        for obj in frame['poi'].get('moving_objects', []):
            # Формируем строку с описанием объекта: координаты (bbox) и площадь (area)
            moving_objects_list.append(f"Object at bbox {obj['bbox']} with area {obj['area']:.2f}")

    # --- Формирование финального описания шота ---
    
    description = {
        'transcription': transcription,  # Текстовая транскрипция
        'sentiment': sentiment,  # Оценка тональности
        'rms': rms,  # RMS аудиосигнала
        'spectral_centroid': spectral_centroid,  # Центр тяжести спектра
        'spectral_bandwidth': spectral_bandwidth,  # Ширина спектра
        'clap_analysis': clap_analysis,  # Анализ звуковых событий CLAP
        'labeled_transcriptions': labeled_transcriptions,  # Распознанные метки в транскрипциях
        'avg_video_objects': avg_detections,  # Средние значения по детекциям объектов
        'avg_events': avg_events,  # Средние значения вероятностей по событиям
        'moving_objects': ', '.join(moving_objects_list)  # Описание движущихся объектов в формате строки
    }
    
    # Возвращаем словарь с полным описанием текущего шота
    return description


# Функция для определения оптимального количества кластеров с помощью метода силуэта
def determine_optimal_clusters_silhouette(data, max_clusters):
    """
    Определяет оптимальное количество кластеров для агломеративной кластеризации с помощью метрики силуэта.
    
    Аргументы:
    data — векторизованные данные для кластеризации (формат: scipy.sparse или numpy.ndarray).
    max_clusters — максимальное количество кластеров для тестирования.

    Возвращает:
    optimal_k — оптимальное количество кластеров, при котором достигается наибольший силуэтный коэффициент.
    Если метрика не удается рассчитать, возвращает значение по умолчанию (2 кластера).
    """

    # Инициализация списка для хранения силуэтных коэффициентов
    silhouette_scores = []

    # Определение диапазона значений количества кластеров (от 2 до максимального значения или количества объектов)
    K = range(2, min(max_clusters + 1, data.shape[0]))  # Число кластеров не должно превышать количество сэмплов

    # Проход по каждому значению k в диапазоне K для оценки силуэтного коэффициента
    for k in K:
        # Создаем модель агломеративной кластеризации с текущим количеством кластеров k
        agglomerative = AgglomerativeClustering(n_clusters=k)

        # Преобразуем данные из sparse-формата в массив (если это необходимо) и выполняем кластеризацию
        labels = agglomerative.fit_predict(data.toarray())

        # Проверка: если кластеризация вернула меньше 2 уникальных кластеров, пропускаем это значение k
        if len(np.unique(labels)) < 2:
            continue  # Пропускаем значение, так как силуэтный коэффициент рассчитывается только для двух и более кластеров

        # Вычисляем силуэтный коэффициент для текущей кластеризации
        score = silhouette_score(data, labels)  # Метрика оценивает, насколько хорошо объекты внутри одного кластера схожи и отделены от других кластеров

        # Сохраняем рассчитанный силуэтный коэффициент в список
        silhouette_scores.append(score)

    # Если после всех проверок нет подходящих силуэтных коэффициентов, возвращаем минимальное значение — 2 кластера
    if len(silhouette_scores) == 0:
        return 2  # Возвращаем минимум 2 кластера по умолчанию
    
    # Определяем значение k, при котором силуэтный коэффициент максимален
    optimal_k = K[np.argmax(silhouette_scores)]  # Выбираем количество кластеров, соответствующее максимальному силуэтному коэффициенту

    # Визуализация результатов для наглядности
    plt.figure(figsize=(8, 4))  # Определяем размер графика
    plt.plot(K, silhouette_scores, 'bx-')  # Строим график: число кластеров против силуэтного коэффициента
    plt.xlabel('Количество кластеров')  # Метка оси X
    plt.ylabel('Силуэтный коэффициент')  # Метка оси Y
    plt.title('Определение оптимального количества кластеров (метод силуэта)')  # Заголовок графика
    plt.show()  # Отображение графика

    # Возвращаем оптимальное количество кластеров
    return optimal_k

# Функция для выполнения агломеративной кластеризации и визуализации результатов
def apply_agglomerative(data, n_clusters):
    """
    Выполняет агломеративную кластеризацию на заданном наборе данных и визуализирует результаты с использованием PCA.

    Аргументы:
    data — векторизованные данные для кластеризации (формат: scipy.sparse или numpy.ndarray).
    n_clusters — количество кластеров для агломеративной кластеризации.

    Возвращает:
    clusters — метки кластеров для каждого объекта (массив, где каждому объекту присвоен номер кластера).
    """
    
    # --- Шаг 1: Применение агломеративной кластеризации ---
    
    # Инициализация модели агломеративной кластеризации с указанным количеством кластеров
    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)
    
    # Выполняем кластеризацию и получаем метки кластеров для каждого объекта
    clusters = agglomerative.fit_predict(data.toarray())  # Преобразуем данные в массив (если это sparse-матрица)

    # --- Шаг 2: Снижение размерности для визуализации с помощью PCA ---
    
    # Инициализация модели PCA для снижения размерности до 2 компонент
    pca = PCA(n_components=2)
    
    # Преобразование исходных данных в двумерное пространство
    reduced_data = pca.fit_transform(data.toarray())  # Преобразуем данные в двумерное пространство для визуализации

    # --- Шаг 3: Визуализация кластеров ---
    
    # Инициализация окна для графика
    plt.figure(figsize=(10, 6))  # Размер окна: 10x6 дюймов
    
    # Построение scatter-графика: x и y — координаты объектов после PCA, цвет точек соответствует меткам кластеров
    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='plasma')  # cmap='plasma' задает цветовую карту
    
    # Добавление цветовой шкалы, указывающей номер кластера
    plt.colorbar(label='Agglomerative Cluster Label')  # Метка для цветовой шкалы
    
    # Настройка осей и заголовка
    plt.xlabel('PCA Component 1')  # Метка оси X
    plt.ylabel('PCA Component 2')  # Метка оси Y
    plt.title('Агломеративная кластеризация с использованием PCA')  # Заголовок графика
    
    # Отображение графика
    plt.show()  # Вывод графика на экран

    # Возвращаем метки кластеров для каждого объекта
    return clusters

# Функция для вычисления и вывода метрик кластеризации
def print_metrics(data, labels, name):
    """
    Вычисляет и выводит метрики качества кластеризации для заданных данных.

    Аргументы:
    data — векторизованные данные для оценки (формат: scipy.sparse или numpy.ndarray).
    labels — метки кластеров для каждого объекта (массив или список).
    name — название метода кластеризации, которое будет использовано в выводе (например, "Agglomerative").

    Возвращает:
    Ничего не возвращает. Выводит метрики на экран.
    """

    # Преобразование разреженной матрицы (если она таковой является) в плотный формат для вычисления метрик
    data_dense = data.toarray()  # Преобразуем данные в массив numpy, так как некоторые метрики не работают с sparse-форматом

    # --- Вычисление метрик качества кластеризации ---
    
    # Вычисление среднего силуэтного коэффициента:
    # Он измеряет, насколько хорошо точки внутри одного кластера сгруппированы и отделены от других кластеров.
    silhouette_avg = silhouette_score(data_dense, labels)  # Чем выше значение (макс. 1.0), тем лучше разделены кластеры

    # Вычисление индекса Дэвиса-Болдена:
    # Он оценивает степень схожести между кластерами. Чем ниже значение, тем лучше (минимальное значение — 0).
    db_index = davies_bouldin_score(data_dense, labels)

    # --- Вывод метрик на экран ---
    
    print(f"{name} Метрики:")  # Заголовок с названием метода кластеризации
    print(f"  Средний силуэтный коэффициент: {silhouette_avg:.4f}")  # Округляем силуэтный коэффициент до 4 знаков после запятой
    print(f"  Индекс Дэвиса-Болдена: {db_index:.4f}")  # Округляем индекс Дэвиса-Болдена до 4 знаков после запятой

def process_and_analyze(audio_file_path, video_file_path, merged_result_file_path):
    """
    Функция для объединения данных аудио и видео шотов, создания текстовых описаний и их кластеризации.

    Аргументы:
    audio_file_path — Путь к JSON-файлу с результатами аудиоанализа.
    video_file_path — Путь к JSON-файлу с результатами видеоанализа.
    merged_result_file_path — Путь для сохранения объединенных и кластеризованных данных.
    """
    
    # Инициализация пустого словаря для хранения объединенных данных
    merged_data = {}
    print(1)
    
    # Загрузка данных из JSON-файла с аудиорезультатами
    with open(audio_file_path, 'r', encoding='utf-8') as f:
        audio_data = json.load(f)  # Загружаем аудиоданные в словарь audio_data
    print(2)
    
    # Загрузка данных из JSON-файла с видеорезультатами
    with open(video_file_path, 'r', encoding='utf-8') as f:
        video_data = json.load(f)  # Загружаем видеоданные в словарь video_data
    print(3)
    
    # Объединение данных аудио и видео по каждому шоту (ключи совпадают)
    for shot_key in audio_data:
        audio_shot = audio_data[shot_key]  # Данные аудио для текущего шота
        video_shot_frames = video_data[shot_key]  # Данные видео для текущего шота
        # Объединяем данные аудио и видео для текущего шота с помощью пользовательской функции merge_shot_data
        merged_data[shot_key] = merge_shot_data(audio_shot, video_shot_frames)
    print(4)
    
    # Создание текстовых описаний на основе объединенных данных для дальнейшей кластеризации
    shot_descriptions = [
        (
            f"Transcription: {shot['transcription']}, "  # Транскрипция текста
            f"Sentiment: {shot['sentiment']}, "  # Анализ тональности
            f"Clap Analysis: {shot['clap_analysis']}, "  # Классификация звуков с помощью CLAP
            f"Average Video Objects: {shot['avg_video_objects']}, "  # Среднее количество объектов в кадре
            f"Average Video Events: {shot['avg_events']}, "  # Среднее количество событий в кадре
        )
        for shot in merged_data.values()
    ]

    print(5)
    
    # Преобразование текстовых описаний в числовые векторы с помощью TF-IDF
    vectorizer = TfidfVectorizer()  # Создаем объект TfidfVectorizer для преобразования текста
    X = vectorizer.fit_transform(shot_descriptions)  # Выполняем векторизацию текстов
    print(6)
    
    # Определение количества шотов
    num_shots = len(shot_descriptions)
    
    # Определяем оптимальное количество кластеров с помощью коэффициента силуэта
    optimal_clusters = determine_optimal_clusters_silhouette(X, num_shots)
    print(7)
    
    # Если оптимальное количество кластеров не определено или меньше 1, устанавливаем минимум в 1 кластер
    if optimal_clusters is None or optimal_clusters < 1:
        optimal_clusters = 1  # Минимальное количество кластеров
    
    # Применяем агломеративную кластеризацию к векторизованным данным
    agglomerative_clusters = apply_agglomerative(X, optimal_clusters)

    # Инициализация словаря для хранения шотов по их кластерным группам
    clusters_dict = {}

    # Проходим по каждому шоту и присваиваем его к определенному кластеру
    for shot_key, description in merged_data.items():
        # Определяем индекс шота и получаем номер его кластера
        cluster_id = int(agglomerative_clusters[list(merged_data.keys()).index(shot_key)])
        
        # Если кластер еще не существует в словаре, создаем его
        if cluster_id not in clusters_dict:
            clusters_dict[cluster_id] = []  # Инициализируем пустой список для текущего кластера
        
        # Добавляем текущий шот в соответствующий кластер
        clusters_dict[cluster_id].append(shot_key)

    # Преобразуем данные кластеров в сериализуемый формат (если необходимо)
    merged_data_serializable = convert_to_serializable(clusters_dict)

    # Выводим метрики для оценки кластеризации
    print_metrics(X, agglomerative_clusters, "Agglomerative")

    # Сохранение результирующих данных кластеров в JSON-файл
    with open(merged_result_file_path, 'w', encoding='utf-8') as f:
        json.dump(merged_data_serializable, f, ensure_ascii=False, indent=4)
